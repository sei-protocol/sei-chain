// Code generated by command: go run edwards_vector.go. DO NOT EDIT.

// +build amd64,!purego,!forcenoasm,!force32bit

#include "textflag.h"

DATA reduce_shifts<>+0(SB)/4, $0x0000001a
DATA reduce_shifts<>+4(SB)/4, $0x0000001a
DATA reduce_shifts<>+8(SB)/4, $0x00000019
DATA reduce_shifts<>+12(SB)/4, $0x00000019
DATA reduce_shifts<>+16(SB)/4, $0x0000001a
DATA reduce_shifts<>+20(SB)/4, $0x0000001a
DATA reduce_shifts<>+24(SB)/4, $0x00000019
DATA reduce_shifts<>+28(SB)/4, $0x00000019
GLOBL reduce_shifts<>(SB), RODATA|NOPTR, $32

DATA reduce_masks<>+0(SB)/4, $0x03ffffff
DATA reduce_masks<>+4(SB)/4, $0x03ffffff
DATA reduce_masks<>+8(SB)/4, $0x01ffffff
DATA reduce_masks<>+12(SB)/4, $0x01ffffff
DATA reduce_masks<>+16(SB)/4, $0x03ffffff
DATA reduce_masks<>+20(SB)/4, $0x03ffffff
DATA reduce_masks<>+24(SB)/4, $0x01ffffff
DATA reduce_masks<>+28(SB)/4, $0x01ffffff
GLOBL reduce_masks<>(SB), RODATA|NOPTR, $32

DATA v19<>+0(SB)/8, $0x0000000000000013
DATA v19<>+8(SB)/8, $0x0000000000000013
DATA v19<>+16(SB)/8, $0x0000000000000013
DATA v19<>+24(SB)/8, $0x0000000000000013
GLOBL v19<>(SB), RODATA|NOPTR, $32

DATA p_times_16_lo<>+0(SB)/4, $0x3ffffed0
DATA p_times_16_lo<>+4(SB)/4, $0x3ffffed0
DATA p_times_16_lo<>+8(SB)/4, $0x1ffffff0
DATA p_times_16_lo<>+12(SB)/4, $0x1ffffff0
DATA p_times_16_lo<>+16(SB)/4, $0x3ffffed0
DATA p_times_16_lo<>+20(SB)/4, $0x3ffffed0
DATA p_times_16_lo<>+24(SB)/4, $0x1ffffff0
DATA p_times_16_lo<>+28(SB)/4, $0x1ffffff0
GLOBL p_times_16_lo<>(SB), RODATA|NOPTR, $32

DATA p_times_16_hi<>+0(SB)/4, $0x3ffffff0
DATA p_times_16_hi<>+4(SB)/4, $0x3ffffff0
DATA p_times_16_hi<>+8(SB)/4, $0x1ffffff0
DATA p_times_16_hi<>+12(SB)/4, $0x1ffffff0
DATA p_times_16_hi<>+16(SB)/4, $0x3ffffff0
DATA p_times_16_hi<>+20(SB)/4, $0x3ffffff0
DATA p_times_16_hi<>+24(SB)/4, $0x1ffffff0
DATA p_times_16_hi<>+28(SB)/4, $0x1ffffff0
GLOBL p_times_16_hi<>(SB), RODATA|NOPTR, $32

DATA p_times_2_lo<>+0(SB)/4, $0x07ffffda
DATA p_times_2_lo<>+4(SB)/4, $0x07ffffda
DATA p_times_2_lo<>+8(SB)/4, $0x03fffffe
DATA p_times_2_lo<>+12(SB)/4, $0x03fffffe
DATA p_times_2_lo<>+16(SB)/4, $0x07ffffda
DATA p_times_2_lo<>+20(SB)/4, $0x07ffffda
DATA p_times_2_lo<>+24(SB)/4, $0x03fffffe
DATA p_times_2_lo<>+28(SB)/4, $0x03fffffe
GLOBL p_times_2_lo<>(SB), RODATA|NOPTR, $32

DATA p_times_2_hi<>+0(SB)/4, $0x07fffffe
DATA p_times_2_hi<>+4(SB)/4, $0x07fffffe
DATA p_times_2_hi<>+8(SB)/4, $0x03fffffe
DATA p_times_2_hi<>+12(SB)/4, $0x03fffffe
DATA p_times_2_hi<>+16(SB)/4, $0x07fffffe
DATA p_times_2_hi<>+20(SB)/4, $0x07fffffe
DATA p_times_2_hi<>+24(SB)/4, $0x03fffffe
DATA p_times_2_hi<>+28(SB)/4, $0x03fffffe
GLOBL p_times_2_hi<>(SB), RODATA|NOPTR, $32

DATA low_25_bit_mask<>+0(SB)/8, $0x0000000001ffffff
DATA low_25_bit_mask<>+8(SB)/8, $0x0000000001ffffff
DATA low_25_bit_mask<>+16(SB)/8, $0x0000000001ffffff
DATA low_25_bit_mask<>+24(SB)/8, $0x0000000001ffffff
GLOBL low_25_bit_mask<>(SB), RODATA|NOPTR, $32

DATA low_26_bit_mask<>+0(SB)/8, $0x0000000003ffffff
DATA low_26_bit_mask<>+8(SB)/8, $0x0000000003ffffff
DATA low_26_bit_mask<>+16(SB)/8, $0x0000000003ffffff
DATA low_26_bit_mask<>+24(SB)/8, $0x0000000003ffffff
GLOBL low_26_bit_mask<>(SB), RODATA|NOPTR, $32

DATA to_cached_scalar<>+0(SB)/4, $0x0001db42
DATA to_cached_scalar<>+4(SB)/4, $0x00000000
DATA to_cached_scalar<>+8(SB)/4, $0x0001db42
DATA to_cached_scalar<>+12(SB)/4, $0x00000000
DATA to_cached_scalar<>+16(SB)/4, $0x0003b684
DATA to_cached_scalar<>+20(SB)/4, $0x00000000
DATA to_cached_scalar<>+24(SB)/4, $0x0003b682
DATA to_cached_scalar<>+28(SB)/4, $0x00000000
GLOBL to_cached_scalar<>(SB), RODATA|NOPTR, $32

DATA low_p_37<>+0(SB)/8, $0x7ffffda000000000
DATA low_p_37<>+8(SB)/8, $0x7ffffda000000000
DATA low_p_37<>+16(SB)/8, $0x7ffffda000000000
DATA low_p_37<>+24(SB)/8, $0x7ffffda000000000
GLOBL low_p_37<>(SB), RODATA|NOPTR, $32

DATA even_p_37<>+0(SB)/8, $0x7fffffe000000000
DATA even_p_37<>+8(SB)/8, $0x7fffffe000000000
DATA even_p_37<>+16(SB)/8, $0x7fffffe000000000
DATA even_p_37<>+24(SB)/8, $0x7fffffe000000000
GLOBL even_p_37<>(SB), RODATA|NOPTR, $32

DATA odd_p_37<>+0(SB)/8, $0x3fffffe000000000
DATA odd_p_37<>+8(SB)/8, $0x3fffffe000000000
DATA odd_p_37<>+16(SB)/8, $0x3fffffe000000000
DATA odd_p_37<>+24(SB)/8, $0x3fffffe000000000
GLOBL odd_p_37<>(SB), RODATA|NOPTR, $32

DATA shuffle_AAAA<>+0(SB)/4, $0x00000000
DATA shuffle_AAAA<>+4(SB)/4, $0x00000000
DATA shuffle_AAAA<>+8(SB)/4, $0x00000002
DATA shuffle_AAAA<>+12(SB)/4, $0x00000002
DATA shuffle_AAAA<>+16(SB)/4, $0x00000000
DATA shuffle_AAAA<>+20(SB)/4, $0x00000000
DATA shuffle_AAAA<>+24(SB)/4, $0x00000002
DATA shuffle_AAAA<>+28(SB)/4, $0x00000002
GLOBL shuffle_AAAA<>(SB), RODATA|NOPTR, $32

DATA shuffle_ABDC<>+0(SB)/4, $0x00000000
DATA shuffle_ABDC<>+4(SB)/4, $0x00000001
DATA shuffle_ABDC<>+8(SB)/4, $0x00000002
DATA shuffle_ABDC<>+12(SB)/4, $0x00000003
DATA shuffle_ABDC<>+16(SB)/4, $0x00000005
DATA shuffle_ABDC<>+20(SB)/4, $0x00000004
DATA shuffle_ABDC<>+24(SB)/4, $0x00000007
DATA shuffle_ABDC<>+28(SB)/4, $0x00000006
GLOBL shuffle_ABDC<>(SB), RODATA|NOPTR, $32

DATA shuffle_ADDA<>+0(SB)/4, $0x00000000
DATA shuffle_ADDA<>+4(SB)/4, $0x00000005
DATA shuffle_ADDA<>+8(SB)/4, $0x00000002
DATA shuffle_ADDA<>+12(SB)/4, $0x00000007
DATA shuffle_ADDA<>+16(SB)/4, $0x00000005
DATA shuffle_ADDA<>+20(SB)/4, $0x00000000
DATA shuffle_ADDA<>+24(SB)/4, $0x00000007
DATA shuffle_ADDA<>+28(SB)/4, $0x00000002
GLOBL shuffle_ADDA<>(SB), RODATA|NOPTR, $32

DATA shuffle_BACD<>+0(SB)/4, $0x00000001
DATA shuffle_BACD<>+4(SB)/4, $0x00000000
DATA shuffle_BACD<>+8(SB)/4, $0x00000003
DATA shuffle_BACD<>+12(SB)/4, $0x00000002
DATA shuffle_BACD<>+16(SB)/4, $0x00000004
DATA shuffle_BACD<>+20(SB)/4, $0x00000005
DATA shuffle_BACD<>+24(SB)/4, $0x00000006
DATA shuffle_BACD<>+28(SB)/4, $0x00000007
GLOBL shuffle_BACD<>(SB), RODATA|NOPTR, $32

DATA shuffle_BBBB<>+0(SB)/4, $0x00000001
DATA shuffle_BBBB<>+4(SB)/4, $0x00000001
DATA shuffle_BBBB<>+8(SB)/4, $0x00000003
DATA shuffle_BBBB<>+12(SB)/4, $0x00000003
DATA shuffle_BBBB<>+16(SB)/4, $0x00000001
DATA shuffle_BBBB<>+20(SB)/4, $0x00000001
DATA shuffle_BBBB<>+24(SB)/4, $0x00000003
DATA shuffle_BBBB<>+28(SB)/4, $0x00000003
GLOBL shuffle_BBBB<>(SB), RODATA|NOPTR, $32

DATA shuffle_CACA<>+0(SB)/4, $0x00000004
DATA shuffle_CACA<>+4(SB)/4, $0x00000000
DATA shuffle_CACA<>+8(SB)/4, $0x00000006
DATA shuffle_CACA<>+12(SB)/4, $0x00000002
DATA shuffle_CACA<>+16(SB)/4, $0x00000004
DATA shuffle_CACA<>+20(SB)/4, $0x00000000
DATA shuffle_CACA<>+24(SB)/4, $0x00000006
DATA shuffle_CACA<>+28(SB)/4, $0x00000002
GLOBL shuffle_CACA<>(SB), RODATA|NOPTR, $32

DATA shuffle_CBCB<>+0(SB)/4, $0x00000004
DATA shuffle_CBCB<>+4(SB)/4, $0x00000001
DATA shuffle_CBCB<>+8(SB)/4, $0x00000006
DATA shuffle_CBCB<>+12(SB)/4, $0x00000003
DATA shuffle_CBCB<>+16(SB)/4, $0x00000004
DATA shuffle_CBCB<>+20(SB)/4, $0x00000001
DATA shuffle_CBCB<>+24(SB)/4, $0x00000006
DATA shuffle_CBCB<>+28(SB)/4, $0x00000003
GLOBL shuffle_CBCB<>(SB), RODATA|NOPTR, $32

DATA shuffle_DBDB<>+0(SB)/4, $0x00000005
DATA shuffle_DBDB<>+4(SB)/4, $0x00000001
DATA shuffle_DBDB<>+8(SB)/4, $0x00000007
DATA shuffle_DBDB<>+12(SB)/4, $0x00000003
DATA shuffle_DBDB<>+16(SB)/4, $0x00000001
DATA shuffle_DBDB<>+20(SB)/4, $0x00000005
DATA shuffle_DBDB<>+24(SB)/4, $0x00000003
DATA shuffle_DBDB<>+28(SB)/4, $0x00000007
GLOBL shuffle_DBDB<>(SB), RODATA|NOPTR, $32

// func vecConditionalSelect_AVX2(out *fieldElement2625x4, a *fieldElement2625x4, b *fieldElement2625x4, mask uint32)
// Requires: AVX, AVX2
TEXT ·vecConditionalSelect_AVX2(SB), NOSPLIT|NOFRAME, $0-32
	MOVQ out+0(FP), AX
	MOVQ a+8(FP), CX
	MOVQ b+16(FP), DX

	// maskVec = [mask, .., mask]
	VPBROADCASTD mask+24(FP), Y0

	// b = b & maskVec
	VPAND (DX), Y0, Y1
	VPAND 32(DX), Y0, Y2
	VPAND 64(DX), Y0, Y3
	VPAND 96(DX), Y0, Y4
	VPAND 128(DX), Y0, Y5

	// tmp = (!a) & maskVec
	VPANDN (CX), Y0, Y6
	VPANDN 32(CX), Y0, Y7
	VPANDN 64(CX), Y0, Y8
	VPANDN 96(CX), Y0, Y9
	VPANDN 128(CX), Y0, Y0

	// b |= tmp
	VPOR Y1, Y6, Y1
	VPOR Y2, Y7, Y2
	VPOR Y3, Y8, Y3
	VPOR Y4, Y9, Y4
	VPOR Y5, Y0, Y5

	// Store output
	VMOVDQU Y1, (AX)
	VMOVDQU Y2, 32(AX)
	VMOVDQU Y3, 64(AX)
	VMOVDQU Y4, 96(AX)
	VMOVDQU Y5, 128(AX)
	VZEROUPPER
	RET

// func vecReduce_AVX2(out *fieldElement2625x4)
// Requires: AVX, AVX2
TEXT ·vecReduce_AVX2(SB), NOSPLIT|NOFRAME, $0-8
	MOVQ out+0(FP), AX

	// Load out
	VMOVDQU (AX), Y0
	VMOVDQU 32(AX), Y1
	VMOVDQU 64(AX), Y2
	VMOVDQU 96(AX), Y3
	VMOVDQU 128(AX), Y4

	// Reduce
	VMOVDQA reduce_shifts<>+0(SB), Y5
	VMOVDQA reduce_masks<>+0(SB), Y6

	// c10, .., c98 = rotated_carryout(v[0]), .., rotated_carryout(v[4])
	VPSRLVD Y5, Y0, Y7
	VPSRLVD Y5, Y1, Y8
	VPSRLVD Y5, Y2, Y9
	VPSRLVD Y5, Y3, Y10
	VPSRLVD Y5, Y4, Y5
	VPSHUFD $0x4e, Y7, Y7
	VPSHUFD $0x4e, Y8, Y8
	VPSHUFD $0x4e, Y9, Y9
	VPSHUFD $0x4e, Y10, Y10
	VPSHUFD $0x4e, Y5, Y5

	// vec &= masks
	VPAND Y6, Y0, Y0
	VPAND Y6, Y1, Y1
	VPAND Y6, Y2, Y2
	VPAND Y6, Y3, Y3
	VPAND Y6, Y4, Y4

	// Combine (lo, .., lo) with (hi, .., hi) to (lo, lo, hi, hi, lo, lo, hi, hi)
	VPXOR    Y6, Y6, Y6
	VPBLENDD $0xcc, Y7, Y6, Y6
	VPBLENDD $0xcc, Y8, Y7, Y7
	VPBLENDD $0xcc, Y9, Y8, Y8
	VPBLENDD $0xcc, Y10, Y9, Y9
	VPBLENDD $0xcc, Y5, Y10, Y10

	// vec += combined
	VPADDD Y0, Y6, Y0
	VPADDD Y1, Y7, Y1
	VPADDD Y2, Y8, Y2
	VPADDD Y3, Y9, Y3
	VPADDD Y4, Y10, Y4

	// vec[0] += c9_19
	VPSHUFD  $0xd8, Y5, Y5
	VPMULUDQ v19<>+0(SB), Y5, Y5
	VPSHUFD  $0xd8, Y5, Y5
	VPADDD   Y0, Y5, Y0

	// Write out the result
	VMOVDQU Y0, (AX)
	VMOVDQU Y1, 32(AX)
	VMOVDQU Y2, 64(AX)
	VMOVDQU Y3, 96(AX)
	VMOVDQU Y4, 128(AX)
	VZEROUPPER
	RET

// func vecNegate_AVX2(out *fieldElement2625x4)
// Requires: AVX, AVX2
TEXT ·vecNegate_AVX2(SB), NOSPLIT|NOFRAME, $0-8
	MOVQ    out+0(FP), AX
	VMOVDQA p_times_16_lo<>+0(SB), Y0
	VMOVDQA p_times_16_hi<>+0(SB), Y1

	// out = p * 16 - out
	VPSUBD (AX), Y0, Y0
	VPSUBD 32(AX), Y1, Y2
	VPSUBD 64(AX), Y1, Y3
	VPSUBD 96(AX), Y1, Y4
	VPSUBD 128(AX), Y1, Y1

	// Reduce
	VMOVDQA reduce_shifts<>+0(SB), Y5
	VMOVDQA reduce_masks<>+0(SB), Y6

	// c10, .., c98 = rotated_carryout(v[0]), .., rotated_carryout(v[4])
	VPSRLVD Y5, Y0, Y7
	VPSRLVD Y5, Y2, Y8
	VPSRLVD Y5, Y3, Y9
	VPSRLVD Y5, Y4, Y10
	VPSRLVD Y5, Y1, Y5
	VPSHUFD $0x4e, Y7, Y7
	VPSHUFD $0x4e, Y8, Y8
	VPSHUFD $0x4e, Y9, Y9
	VPSHUFD $0x4e, Y10, Y10
	VPSHUFD $0x4e, Y5, Y5

	// vec &= masks
	VPAND Y6, Y0, Y0
	VPAND Y6, Y2, Y2
	VPAND Y6, Y3, Y3
	VPAND Y6, Y4, Y4
	VPAND Y6, Y1, Y1

	// Combine (lo, .., lo) with (hi, .., hi) to (lo, lo, hi, hi, lo, lo, hi, hi)
	VPXOR    Y6, Y6, Y6
	VPBLENDD $0xcc, Y7, Y6, Y6
	VPBLENDD $0xcc, Y8, Y7, Y7
	VPBLENDD $0xcc, Y9, Y8, Y8
	VPBLENDD $0xcc, Y10, Y9, Y9
	VPBLENDD $0xcc, Y5, Y10, Y10

	// vec += combined
	VPADDD Y0, Y6, Y0
	VPADDD Y2, Y7, Y2
	VPADDD Y3, Y8, Y3
	VPADDD Y4, Y9, Y4
	VPADDD Y1, Y10, Y1

	// vec[0] += c9_19
	VPSHUFD  $0xd8, Y5, Y5
	VPMULUDQ v19<>+0(SB), Y5, Y5
	VPSHUFD  $0xd8, Y5, Y5
	VPADDD   Y0, Y5, Y0

	// Write out the result
	VMOVDQU Y0, (AX)
	VMOVDQU Y2, 32(AX)
	VMOVDQU Y3, 64(AX)
	VMOVDQU Y4, 96(AX)
	VMOVDQU Y1, 128(AX)
	VZEROUPPER
	RET

// func vecAddSubExtendedCached_Step1_AVX2(out *fieldElement2625x4, vec *extendedPoint)
// Requires: AVX, AVX2
TEXT ·vecAddSubExtendedCached_Step1_AVX2(SB), NOSPLIT|NOFRAME, $0-16
	MOVQ    out+0(FP), AX
	MOVQ    vec+8(FP), CX
	VMOVDQU (CX), Y0
	VMOVDQU 32(CX), Y1
	VMOVDQU 64(CX), Y2
	VMOVDQU 96(CX), Y3
	VMOVDQU 128(CX), Y4

	// tmp = vec.diff_sum()

	// tmp1 = vec.shuffle(BADC)
	VPSHUFD $0xb1, Y0, Y5
	VPSHUFD $0xb1, Y1, Y6
	VPSHUFD $0xb1, Y2, Y7
	VPSHUFD $0xb1, Y3, Y8
	VPSHUFD $0xb1, Y4, Y9

	// tmp2 = vec.negate_lazy()
	VMOVDQA p_times_2_lo<>+0(SB), Y10
	VMOVDQA p_times_2_hi<>+0(SB), Y11
	VPSUBD  Y0, Y10, Y10
	VPSUBD  Y1, Y11, Y12
	VPSUBD  Y2, Y11, Y13
	VPSUBD  Y3, Y11, Y14
	VPSUBD  Y4, Y11, Y11

	// tmp2 = vec.blend(tmp2, Lanes::AC)
	VPBLENDD $0x55, Y10, Y0, Y10
	VPBLENDD $0x55, Y12, Y1, Y12
	VPBLENDD $0x55, Y13, Y2, Y13
	VPBLENDD $0x55, Y14, Y3, Y14
	VPBLENDD $0x55, Y11, Y4, Y11

	// tmp = tmp1 + tmp2 (diff_sum result)
	VPADDD Y5, Y10, Y5
	VPADDD Y6, Y12, Y6
	VPADDD Y7, Y13, Y7
	VPADDD Y8, Y14, Y8
	VPADDD Y9, Y11, Y9

	// out = vec.blend(tmp, Lanes::AB)
	VPBLENDD $0x0f, Y5, Y0, Y0
	VPBLENDD $0x0f, Y6, Y1, Y1
	VPBLENDD $0x0f, Y7, Y2, Y2
	VPBLENDD $0x0f, Y8, Y3, Y3
	VPBLENDD $0x0f, Y9, Y4, Y4

	// Write out the result
	VMOVDQU Y0, (AX)
	VMOVDQU Y1, 32(AX)
	VMOVDQU Y2, 64(AX)
	VMOVDQU Y3, 96(AX)
	VMOVDQU Y4, 128(AX)
	VZEROUPPER
	RET

// func vecAddSubExtendedCached_Step2_AVX2(tmp0 *fieldElement2625x4, tmp1 *fieldElement2625x4)
// Requires: AVX, AVX2
TEXT ·vecAddSubExtendedCached_Step2_AVX2(SB), NOSPLIT|NOFRAME, $0-16
	MOVQ    tmp0+0(FP), AX
	MOVQ    tmp1+8(FP), CX
	VMOVDQU (AX), Y0
	VMOVDQU 32(AX), Y1
	VMOVDQU 64(AX), Y2
	VMOVDQU 96(AX), Y3
	VMOVDQU 128(AX), Y4

	// tmp = tmp0.shuffle(Shuffle::ABDC)
	VMOVDQA shuffle_ABDC<>+0(SB), Y5
	VPERMD  Y0, Y5, Y0
	VPERMD  Y1, Y5, Y1
	VPERMD  Y2, Y5, Y2
	VPERMD  Y3, Y5, Y3
	VPERMD  Y4, Y5, Y4

	// tmp = tmp.diff_sum()

	// tmp1 = tmp.shuffle(BADC)
	VPSHUFD $0xb1, Y0, Y5
	VPSHUFD $0xb1, Y1, Y6
	VPSHUFD $0xb1, Y2, Y7
	VPSHUFD $0xb1, Y3, Y8
	VPSHUFD $0xb1, Y4, Y9

	// tmp2 = tmp.negate_lazy()
	VMOVDQA p_times_2_lo<>+0(SB), Y10
	VMOVDQA p_times_2_hi<>+0(SB), Y11
	VPSUBD  Y0, Y10, Y10
	VPSUBD  Y1, Y11, Y12
	VPSUBD  Y2, Y11, Y13
	VPSUBD  Y3, Y11, Y14
	VPSUBD  Y4, Y11, Y11

	// tmp2 = tmp.blend(tmp2, Lanes::AC)
	VPBLENDD $0x55, Y10, Y0, Y10
	VPBLENDD $0x55, Y12, Y1, Y12
	VPBLENDD $0x55, Y13, Y2, Y13
	VPBLENDD $0x55, Y14, Y3, Y14
	VPBLENDD $0x55, Y11, Y4, Y11

	// tmp = tmp1 + tmp2 (diff_sum result)
	VPADDD Y5, Y10, Y5
	VPADDD Y6, Y12, Y6
	VPADDD Y7, Y13, Y7
	VPADDD Y8, Y14, Y8
	VPADDD Y9, Y11, Y9

	// t0 = tmp.shuffle(Shuffle::ADDA)
	VMOVDQA shuffle_ADDA<>+0(SB), Y4
	VPERMD  Y5, Y4, Y0
	VPERMD  Y6, Y4, Y1
	VPERMD  Y7, Y4, Y2
	VPERMD  Y8, Y4, Y3
	VPERMD  Y9, Y4, Y4

	// t1 = tmp.shuffle(Shuffle::CBCB
	VMOVDQA shuffle_CBCB<>+0(SB), Y10
	VPERMD  Y5, Y10, Y5
	VPERMD  Y6, Y10, Y6
	VPERMD  Y7, Y10, Y7
	VPERMD  Y8, Y10, Y8
	VPERMD  Y9, Y10, Y9

	// Write out t0
	VMOVDQU Y0, (AX)
	VMOVDQU Y1, 32(AX)
	VMOVDQU Y2, 64(AX)
	VMOVDQU Y3, 96(AX)
	VMOVDQU Y4, 128(AX)

	// Write out t1
	VMOVDQU Y5, (CX)
	VMOVDQU Y6, 32(CX)
	VMOVDQU Y7, 64(CX)
	VMOVDQU Y8, 96(CX)
	VMOVDQU Y9, 128(CX)
	VZEROUPPER
	RET

// func vecNegateLazyCached_AVX2(out *fieldElement2625x4, vec *cachedPoint)
// Requires: AVX, AVX2
TEXT ·vecNegateLazyCached_AVX2(SB), NOSPLIT|NOFRAME, $0-16
	MOVQ    out+0(FP), AX
	MOVQ    vec+8(FP), CX
	VMOVDQU (CX), Y0
	VMOVDQU 32(CX), Y1
	VMOVDQU 64(CX), Y2
	VMOVDQU 96(CX), Y3
	VMOVDQU 128(CX), Y4

	// swapped = vec.shuffle(Shuffle::BACD)
	VMOVDQA shuffle_BACD<>+0(SB), Y5
	VPERMD  Y0, Y5, Y0
	VPERMD  Y1, Y5, Y1
	VPERMD  Y2, Y5, Y2
	VPERMD  Y3, Y5, Y3
	VPERMD  Y4, Y5, Y4

	// tmp = swapped.negate_lazy()
	VMOVDQA p_times_2_lo<>+0(SB), Y5
	VMOVDQA p_times_2_hi<>+0(SB), Y6
	VPSUBD  Y0, Y5, Y5
	VPSUBD  Y1, Y6, Y7
	VPSUBD  Y2, Y6, Y8
	VPSUBD  Y3, Y6, Y9
	VPSUBD  Y4, Y6, Y6

	// out = swapped.blend(swapped.NegateLazy(), Lanes::D
	VPBLENDD $0xa0, Y5, Y0, Y0
	VPBLENDD $0xa0, Y7, Y1, Y1
	VPBLENDD $0xa0, Y8, Y2, Y2
	VPBLENDD $0xa0, Y9, Y3, Y3
	VPBLENDD $0xa0, Y6, Y4, Y4

	// Write out the result
	VMOVDQU Y0, (AX)
	VMOVDQU Y1, 32(AX)
	VMOVDQU Y2, 64(AX)
	VMOVDQU Y3, 96(AX)
	VMOVDQU Y4, 128(AX)
	VZEROUPPER
	RET

// func vecCachedFromExtended_Step1_AVX2(out *cachedPoint, vec *extendedPoint)
// Requires: AVX, AVX2
TEXT ·vecCachedFromExtended_Step1_AVX2(SB), NOSPLIT|NOFRAME, $0-16
	MOVQ    out+0(FP), AX
	MOVQ    vec+8(FP), CX
	VMOVDQU (CX), Y0
	VMOVDQU 32(CX), Y1
	VMOVDQU 64(CX), Y2
	VMOVDQU 96(CX), Y3
	VMOVDQU 128(CX), Y4

	// x = vec

	// tmp = x.diff_sum()

	// tmp1 = x.shuffle(BADC)
	VPSHUFD $0xb1, Y0, Y5
	VPSHUFD $0xb1, Y1, Y6
	VPSHUFD $0xb1, Y2, Y7
	VPSHUFD $0xb1, Y3, Y8
	VPSHUFD $0xb1, Y4, Y9

	// tmp2 = x.negate_lazy()
	VMOVDQA p_times_2_lo<>+0(SB), Y10
	VMOVDQA p_times_2_hi<>+0(SB), Y11
	VPSUBD  Y0, Y10, Y10
	VPSUBD  Y1, Y11, Y12
	VPSUBD  Y2, Y11, Y13
	VPSUBD  Y3, Y11, Y14
	VPSUBD  Y4, Y11, Y11

	// tmp2 = x.blend(tmp2, Lanes::AC)
	VPBLENDD $0x55, Y10, Y0, Y10
	VPBLENDD $0x55, Y12, Y1, Y12
	VPBLENDD $0x55, Y13, Y2, Y13
	VPBLENDD $0x55, Y14, Y3, Y14
	VPBLENDD $0x55, Y11, Y4, Y11

	// tmp = tmp1 + tmp2 (diff_sum result)
	VPADDD Y5, Y10, Y5
	VPADDD Y6, Y12, Y6
	VPADDD Y7, Y13, Y7
	VPADDD Y8, Y14, Y8
	VPADDD Y9, Y11, Y9

	// x = x.blend(tmp, LANES::AB)
	VPBLENDD $0x0f, Y5, Y0, Y0
	VPBLENDD $0x0f, Y6, Y1, Y1
	VPBLENDD $0x0f, Y7, Y2, Y2
	VPBLENDD $0x0f, Y8, Y3, Y3
	VPBLENDD $0x0f, Y9, Y4, Y4

	// x = x * (121666, 121666, 2 * 121666, 2 * 121665)

	// Unpack x
	VPXOR      Y5, Y5, Y5
	VPUNPCKHDQ Y5, Y0, Y6
	VPUNPCKLDQ Y5, Y0, Y0
	VPUNPCKHDQ Y5, Y1, Y7
	VPUNPCKLDQ Y5, Y1, Y1
	VPUNPCKHDQ Y5, Y2, Y8
	VPUNPCKLDQ Y5, Y2, Y2
	VPUNPCKHDQ Y5, Y3, Y9
	VPUNPCKLDQ Y5, Y3, Y3
	VPUNPCKHDQ Y5, Y4, Y10
	VPUNPCKLDQ Y5, Y4, Y4

	// Multiply x by the constant
	VMOVDQA  to_cached_scalar<>+0(SB), Y5
	VPMULUDQ Y5, Y0, Y0
	VPMULUDQ Y5, Y6, Y6
	VPMULUDQ Y5, Y1, Y1
	VPMULUDQ Y5, Y7, Y7
	VPMULUDQ Y5, Y2, Y2
	VPMULUDQ Y5, Y8, Y8
	VPMULUDQ Y5, Y3, Y3
	VPMULUDQ Y5, Y9, Y9
	VPMULUDQ Y5, Y4, Y4
	VPMULUDQ Y5, Y10, Y10

	// Reduce
	VMOVDQA low_25_bit_mask<>+0(SB), Y5
	VMOVDQA low_26_bit_mask<>+0(SB), Y11
	VMOVDQA v19<>+0(SB), Y12

	// Perform two halves of the carry chain in parallel

	// Carry z[0]/z[4]
	VPSRLQ $0x1a, Y0, Y13
	VPSRLQ $0x1a, Y2, Y14
	VPADDQ Y6, Y13, Y6
	VPADDQ Y8, Y14, Y8
	VPAND  Y11, Y0, Y0
	VPAND  Y11, Y2, Y2

	// Carry z[1]/z[5]
	VPSRLQ $0x19, Y6, Y13
	VPSRLQ $0x19, Y8, Y14
	VPADDQ Y1, Y13, Y1
	VPADDQ Y3, Y14, Y3
	VPAND  Y5, Y6, Y6
	VPAND  Y5, Y8, Y8

	// Carry z[2]/z[6]
	VPSRLQ $0x1a, Y1, Y13
	VPSRLQ $0x1a, Y3, Y14
	VPADDQ Y7, Y13, Y7
	VPADDQ Y9, Y14, Y9
	VPAND  Y11, Y1, Y1
	VPAND  Y11, Y3, Y3

	// Carry z[3]/z[7]
	VPSRLQ $0x19, Y7, Y13
	VPSRLQ $0x19, Y9, Y14
	VPADDQ Y2, Y13, Y2
	VPADDQ Y4, Y14, Y4
	VPAND  Y5, Y7, Y7
	VPAND  Y5, Y9, Y9

	// Carry z[4]/z[8]
	VPSRLQ $0x1a, Y2, Y13
	VPSRLQ $0x1a, Y4, Y14
	VPADDQ Y8, Y13, Y8
	VPADDQ Y10, Y14, Y10
	VPAND  Y11, Y2, Y2
	VPAND  Y11, Y4, Y4

	// Do the final carry
	VPSRLQ   $0x19, Y10, Y13
	VPAND    Y5, Y10, Y10
	VPAND    Y11, Y13, Y5
	VPSRLQ   $0x1a, Y13, Y13
	VPMULUDQ Y12, Y5, Y5
	VPMULUDQ Y12, Y13, Y13
	VPADDQ   Y0, Y5, Y0
	VPADDQ   Y6, Y13, Y6
	VPSRLQ   $0x1a, Y0, Y5
	VPADDQ   Y6, Y5, Y6
	VPAND    Y11, Y0, Y0

	// Repack 64-bit lanes into 32-bit lanes
	VPSHUFD  $0xd8, Y0, Y0
	VPSHUFD  $0x8d, Y6, Y6
	VPBLENDD $0xcc, Y6, Y0, Y0
	VPSHUFD  $0xd8, Y1, Y1
	VPSHUFD  $0x8d, Y7, Y7
	VPBLENDD $0xcc, Y7, Y1, Y1
	VPSHUFD  $0xd8, Y2, Y2
	VPSHUFD  $0x8d, Y8, Y8
	VPBLENDD $0xcc, Y8, Y2, Y2
	VPSHUFD  $0xd8, Y3, Y3
	VPSHUFD  $0x8d, Y9, Y9
	VPBLENDD $0xcc, Y9, Y3, Y3
	VPSHUFD  $0xd8, Y4, Y4
	VPSHUFD  $0x8d, Y10, Y10
	VPBLENDD $0xcc, Y10, Y4, Y4

	// Write out the result
	VMOVDQU Y0, (AX)
	VMOVDQU Y1, 32(AX)
	VMOVDQU Y2, 64(AX)
	VMOVDQU Y3, 96(AX)
	VMOVDQU Y4, 128(AX)
	VZEROUPPER
	RET

// func vecDoubleExtended_Step1_AVX2(out *fieldElement2625x4, vec *extendedPoint)
// Requires: AVX, AVX2
TEXT ·vecDoubleExtended_Step1_AVX2(SB), NOSPLIT|NOFRAME, $0-16
	MOVQ    out+0(FP), AX
	MOVQ    vec+8(FP), CX
	VMOVDQU (CX), Y0
	VMOVDQU 32(CX), Y1
	VMOVDQU 64(CX), Y2
	VMOVDQU 96(CX), Y3
	VMOVDQU 128(CX), Y4

	// tmp0 = vec.shuffle(Shuffle::ABAB) (tmp0 = (X1 Y1 X1 Y1)
	VPERMQ $0x44, Y0, Y5
	VPERMQ $0x44, Y1, Y6
	VPERMQ $0x44, Y2, Y7
	VPERMQ $0x44, Y3, Y8
	VPERMQ $0x44, Y4, Y9

	// tmp1 = tmp0.shuffle(Shuffle::BADC) (tmp1 = (Y1 X1 Y1 X1)
	VPSHUFD $0xb1, Y5, Y10
	VPSHUFD $0xb1, Y6, Y11
	VPSHUFD $0xb1, Y7, Y12
	VPSHUFD $0xb1, Y8, Y13
	VPSHUFD $0xb1, Y9, Y14

	// tmp = tmp0 + tmp1
	VPADDD Y5, Y10, Y5
	VPADDD Y6, Y11, Y6
	VPADDD Y7, Y12, Y7
	VPADDD Y8, Y13, Y8
	VPADDD Y9, Y14, Y9

	// tmp0 = vec.blend(tmp, Lanes::D)
	VPBLENDD $0xa0, Y5, Y0, Y5
	VPBLENDD $0xa0, Y6, Y1, Y6
	VPBLENDD $0xa0, Y7, Y2, Y7
	VPBLENDD $0xa0, Y8, Y3, Y8
	VPBLENDD $0xa0, Y9, Y4, Y9

	// Write out the result
	VMOVDQU Y5, (AX)
	VMOVDQU Y6, 32(AX)
	VMOVDQU Y7, 64(AX)
	VMOVDQU Y8, 96(AX)
	VMOVDQU Y9, 128(AX)
	VZEROUPPER
	RET

// func vecDoubleExtended_Step2_AVX2(tmp0 *fieldElement2625x4, tmp1 *fieldElement2625x4)
// Requires: AVX, AVX2
TEXT ·vecDoubleExtended_Step2_AVX2(SB), NOSPLIT|NOFRAME, $0-16
	MOVQ    tmp0+0(FP), AX
	MOVQ    tmp1+8(FP), CX
	VPXOR   Y0, Y0, Y0
	VMOVDQU (CX), Y1
	VMOVDQU 32(CX), Y2
	VMOVDQU 64(CX), Y3
	VMOVDQU 96(CX), Y4
	VMOVDQU 128(CX), Y5

	// tmp = tmp1 + tmp1
	VPADDD Y1, Y1, Y6
	VPADDD Y2, Y2, Y7
	VPADDD Y3, Y3, Y8
	VPADDD Y4, Y4, Y9
	VPADDD Y5, Y5, Y10

	// tmp0 = zero.blend(tmp, Lanes::C)
	VPBLENDD $0x50, Y6, Y0, Y11
	VPBLENDD $0x50, Y7, Y0, Y12
	VPBLENDD $0x50, Y8, Y0, Y13
	VPBLENDD $0x50, Y9, Y0, Y14
	VPBLENDD $0x50, Y10, Y0, Y15

	// tmp0 = tmp0.blend(tmp1, Lanes::D)
	VPBLENDD $0xa0, Y1, Y11, Y11
	VPBLENDD $0xa0, Y2, Y12, Y12
	VPBLENDD $0xa0, Y3, Y13, Y13
	VPBLENDD $0xa0, Y4, Y14, Y14
	VPBLENDD $0xa0, Y5, Y15, Y15

	// S_1 = tmp1.shuffle(Shuffle::AAAA)
	VMOVDQA shuffle_AAAA<>+0(SB), Y10
	VPERMD  Y1, Y10, Y6
	VPERMD  Y2, Y10, Y7
	VPERMD  Y3, Y10, Y8
	VPERMD  Y4, Y10, Y9
	VPERMD  Y5, Y10, Y10

	// tmp0 = tmp0 + S_1
	VPADDD Y11, Y6, Y11
	VPADDD Y12, Y7, Y12
	VPADDD Y13, Y8, Y13
	VPADDD Y14, Y9, Y14
	VPADDD Y15, Y10, Y15

	// S_2 = tmp1.shuffle(Shuffle::BBBB)
	VMOVDQA shuffle_BBBB<>+0(SB), Y6
	VPERMD  Y1, Y6, Y1
	VPERMD  Y2, Y6, Y2
	VPERMD  Y3, Y6, Y3
	VPERMD  Y4, Y6, Y4
	VPERMD  Y5, Y6, Y5

	// tmp = zero.blend(S_2, Lanes::AD)
	VPBLENDD $0xa5, Y1, Y0, Y6
	VPBLENDD $0xa5, Y2, Y0, Y7
	VPBLENDD $0xa5, Y3, Y0, Y8
	VPBLENDD $0xa5, Y4, Y0, Y9
	VPBLENDD $0xa5, Y5, Y0, Y10

	// tmp0 = tmp0 + tmp
	VPADDD Y11, Y6, Y11
	VPADDD Y12, Y7, Y12
	VPADDD Y13, Y8, Y13
	VPADDD Y14, Y9, Y14
	VPADDD Y15, Y10, Y15

	// tmp = S_2.negate_lazy()
	VMOVDQA p_times_2_lo<>+0(SB), Y6
	VMOVDQA p_times_2_hi<>+0(SB), Y7
	VPSUBD  Y1, Y6, Y1
	VPSUBD  Y2, Y7, Y2
	VPSUBD  Y3, Y7, Y3
	VPSUBD  Y4, Y7, Y4
	VPSUBD  Y5, Y7, Y5

	// tmp = zero.blend(tmp, Lanes::BC)
	VPBLENDD $0x5a, Y1, Y0, Y1
	VPBLENDD $0x5a, Y2, Y0, Y2
	VPBLENDD $0x5a, Y3, Y0, Y3
	VPBLENDD $0x5a, Y4, Y0, Y4
	VPBLENDD $0x5a, Y5, Y0, Y5

	// tmp0 = tmp0 + tmp
	VPADDD Y11, Y1, Y11
	VPADDD Y12, Y2, Y12
	VPADDD Y13, Y3, Y13
	VPADDD Y14, Y4, Y14
	VPADDD Y15, Y5, Y15

	// tmp1 = tmp0.shuffle(Shuffle::DBBD)
	VMOVDQA shuffle_DBDB<>+0(SB), Y4
	VPERMD  Y11, Y4, Y0
	VPERMD  Y12, Y4, Y1
	VPERMD  Y13, Y4, Y2
	VPERMD  Y14, Y4, Y3
	VPERMD  Y15, Y4, Y4

	// tmp0 = tmp0.shuffle(Shuffle::CACA)
	VMOVDQA shuffle_CACA<>+0(SB), Y5
	VPERMD  Y11, Y5, Y6
	VPERMD  Y12, Y5, Y7
	VPERMD  Y13, Y5, Y8
	VPERMD  Y14, Y5, Y9
	VPERMD  Y15, Y5, Y5

	// Write out tmp0
	VMOVDQU Y6, (AX)
	VMOVDQU Y7, 32(AX)
	VMOVDQU Y8, 64(AX)
	VMOVDQU Y9, 96(AX)
	VMOVDQU Y5, 128(AX)

	// Write out tmp1
	VMOVDQU Y0, (CX)
	VMOVDQU Y1, 32(CX)
	VMOVDQU Y2, 64(CX)
	VMOVDQU Y3, 96(CX)
	VMOVDQU Y4, 128(CX)
	VZEROUPPER
	RET

// func vecMul_AVX2(out *fieldElement2625x4, a *fieldElement2625x4, b *fieldElement2625x4)
// Requires: AVX, AVX2
TEXT ·vecMul_AVX2(SB), $960-24
	MOVQ out+0(FP), AX

	// Align the stack on a 64 byte boundary (cache line aligned)
	MOVQ SP, CX
	ADDQ $0x40, CX
	ANDQ $0xffffffc0, CX

	// Load, unpack, and spill a (x)
	MOVQ       a+8(FP), DX
	VPXOR      Y10, Y10, Y10
	VMOVDQU    (DX), Y0
	VPUNPCKHDQ Y10, Y0, Y1
	VPUNPCKLDQ Y10, Y0, Y0
	VMOVDQU    32(DX), Y2
	VPUNPCKHDQ Y10, Y2, Y3
	VPUNPCKLDQ Y10, Y2, Y2
	VMOVDQU    64(DX), Y4
	VPUNPCKHDQ Y10, Y4, Y5
	VPUNPCKLDQ Y10, Y4, Y4
	VMOVDQU    96(DX), Y6
	VPUNPCKHDQ Y10, Y6, Y7
	VPUNPCKLDQ Y10, Y6, Y6
	VMOVDQU    128(DX), Y8
	VPUNPCKHDQ Y10, Y8, Y9
	VPUNPCKLDQ Y10, Y8, Y8
	VMOVDQU    Y0, 160(CX)
	VMOVDQU    Y1, 192(CX)
	VMOVDQU    Y2, 224(CX)
	VMOVDQU    Y3, 256(CX)
	VMOVDQU    Y4, 288(CX)
	VMOVDQU    Y5, 320(CX)
	VMOVDQU    Y6, 352(CX)
	VMOVDQU    Y7, 384(CX)
	VMOVDQU    Y8, 416(CX)
	VMOVDQU    Y9, 448(CX)

	// Load, unpack, and spill b (y)
	MOVQ       b+16(FP), DX
	VPXOR      Y10, Y10, Y10
	VMOVDQU    (DX), Y0
	VPUNPCKHDQ Y10, Y0, Y1
	VPUNPCKLDQ Y10, Y0, Y0
	VMOVDQU    32(DX), Y2
	VPUNPCKHDQ Y10, Y2, Y3
	VPUNPCKLDQ Y10, Y2, Y2
	VMOVDQU    64(DX), Y4
	VPUNPCKHDQ Y10, Y4, Y5
	VPUNPCKLDQ Y10, Y4, Y4
	VMOVDQU    96(DX), Y6
	VPUNPCKHDQ Y10, Y6, Y7
	VPUNPCKLDQ Y10, Y6, Y6
	VMOVDQU    128(DX), Y8
	VPUNPCKHDQ Y10, Y8, Y9
	VPUNPCKLDQ Y10, Y8, Y8
	VMOVDQU    Y0, 480(CX)
	VMOVDQU    Y1, 512(CX)
	VMOVDQU    Y2, 544(CX)
	VMOVDQU    Y3, 576(CX)
	VMOVDQU    Y4, 608(CX)
	VMOVDQU    Y5, 640(CX)
	VMOVDQU    Y6, 672(CX)
	VMOVDQU    Y7, 704(CX)
	VMOVDQU    Y8, 736(CX)
	VMOVDQU    Y9, 768(CX)

	// Precompute (y1 .. y9) * 19
	VMOVDQA  v19<>+0(SB), Y0
	VPMULUDQ Y1, Y0, Y1
	VPMULUDQ Y2, Y0, Y2
	VPMULUDQ Y3, Y0, Y3
	VPMULUDQ Y4, Y0, Y4
	VPMULUDQ Y5, Y0, Y10
	VPMULUDQ Y6, Y0, Y11
	VPMULUDQ Y7, Y0, Y12
	VPMULUDQ Y8, Y0, Y13
	VPMULUDQ Y9, Y0, Y14
	VMOVDQA  Y2, 800(CX)
	VMOVDQA  Y3, 832(CX)
	VMOVDQA  Y4, 864(CX)

	// Handle even z vectors

	// z0 = m(x9_2,y1_19)
	// z2 = m(x9_2,y3_19)
	// z4 = m(x9_2,y5_19)
	// z6 = m(x9_2,y7_19)
	// z8 = m(x9_2,y9_19)
	VMOVDQA  448(CX), Y5
	VPADDD   Y5, Y5, Y5
	VPMULUDQ Y1, Y5, Y0
	VPMULUDQ Y3, Y5, Y2
	VPMULUDQ Y10, Y5, Y4
	VPMULUDQ Y12, Y5, Y6
	VPMULUDQ Y14, Y5, Y8

	// z0 += m(x8,y2_19)
	// z2 += m(x8,y4_19)
	// z4 += m(x8,y6_19)
	// z6 += m(x8,y8_19)
	// z8 += m(x8,y0)
	VMOVDQA  416(CX), Y9
	VPMULUDQ 800(CX), Y9, Y1
	VPMULUDQ 864(CX), Y9, Y3
	VPMULUDQ Y11, Y9, Y5
	VPMULUDQ Y13, Y9, Y7
	VPMULUDQ 480(CX), Y9, Y9
	VPADDQ   Y0, Y1, Y0
	VPADDQ   Y2, Y3, Y2
	VPADDQ   Y4, Y5, Y4
	VPADDQ   Y6, Y7, Y6
	VPADDQ   Y8, Y9, Y8

	// z0 += m(x7_2,y3_19)
	// z2 += m(x7_2,y5_19)
	// z4 += m(x7_2,y7_19)
	// z6 += m(x7_2,y9_19)
	// z8 += m(x7_2,y1)
	VMOVDQA  384(CX), Y9
	VPADDD   Y9, Y9, Y9
	VPMULUDQ 832(CX), Y9, Y1
	VPMULUDQ Y10, Y9, Y3
	VPMULUDQ Y12, Y9, Y5
	VPMULUDQ Y14, Y9, Y7
	VPMULUDQ 512(CX), Y9, Y9
	VPADDQ   Y0, Y1, Y0
	VPADDQ   Y2, Y3, Y2
	VPADDQ   Y4, Y5, Y4
	VPADDQ   Y6, Y7, Y6
	VPADDQ   Y8, Y9, Y8

	// z0 += m(x6,y4_19)
	// z2 += m(x6,y6_19)
	// z4 += m(x6,y8_19)
	// z6 += m(x6,y0)
	// z8 += m(x6,y2)
	VMOVDQA  352(CX), Y9
	VPMULUDQ 864(CX), Y9, Y1
	VPMULUDQ Y11, Y9, Y3
	VPMULUDQ Y13, Y9, Y5
	VPMULUDQ 480(CX), Y9, Y7
	VPMULUDQ 544(CX), Y9, Y9
	VPADDQ   Y0, Y1, Y0
	VPADDQ   Y2, Y3, Y2
	VPADDQ   Y4, Y5, Y4
	VPADDQ   Y6, Y7, Y6
	VPADDQ   Y8, Y9, Y8

	// z0 += m(x5_2,y5_19)
	// z2 += m(x5_2,y7_19)
	// z4 += m(x5_2,y9_19)
	// z6 += m(x5_2,y1)
	// z8 += m(x5_2,y3)
	VMOVDQA  320(CX), Y9
	VPADDD   Y9, Y9, Y9
	VPMULUDQ Y10, Y9, Y1
	VPMULUDQ Y12, Y9, Y3
	VPMULUDQ Y14, Y9, Y5
	VPMULUDQ 512(CX), Y9, Y7
	VPMULUDQ 576(CX), Y9, Y9
	VPADDQ   Y0, Y1, Y0
	VPADDQ   Y2, Y3, Y2
	VPADDQ   Y4, Y5, Y4
	VPADDQ   Y6, Y7, Y6
	VPADDQ   Y8, Y9, Y8

	// z0 += m(x4,y6_19)
	// z2 += m(x4,y8_19)
	// z4 += m(x4,y0)
	// z6 += m(x4,y2)
	// z8 += m(x4,y4)
	VMOVDQA  288(CX), Y9
	VPMULUDQ Y11, Y9, Y1
	VPMULUDQ Y13, Y9, Y3
	VPMULUDQ 480(CX), Y9, Y5
	VPMULUDQ 544(CX), Y9, Y7
	VPMULUDQ 608(CX), Y9, Y9
	VPADDQ   Y0, Y1, Y0
	VPADDQ   Y2, Y3, Y2
	VPADDQ   Y4, Y5, Y4
	VPADDQ   Y6, Y7, Y6
	VPADDQ   Y8, Y9, Y8

	// z0 += m(x3_2,y7_19)
	// z2 += m(x3_2,y9_19)
	// z4 += m(x3_2,y1)
	// z6 += m(x3_2,y3)
	// z8 += m(x3_2,y5)
	VMOVDQA  256(CX), Y9
	VPADDD   Y9, Y9, Y9
	VPMULUDQ Y12, Y9, Y1
	VPMULUDQ Y14, Y9, Y3
	VPMULUDQ 512(CX), Y9, Y5
	VPMULUDQ 576(CX), Y9, Y7
	VPMULUDQ 640(CX), Y9, Y9
	VPADDQ   Y0, Y1, Y0
	VPADDQ   Y2, Y3, Y2
	VPADDQ   Y4, Y5, Y4
	VPADDQ   Y6, Y7, Y6
	VPADDQ   Y8, Y9, Y8

	// z0 += m(x2,y8_19)
	// z2 += m(x2,y0)
	// z4 += m(x2,y2)
	// z6 += m(x2,y4)
	// z8 += m(x2,y6)
	VMOVDQA  224(CX), Y9
	VPMULUDQ Y13, Y9, Y1
	VPMULUDQ 480(CX), Y9, Y3
	VPMULUDQ 544(CX), Y9, Y5
	VPMULUDQ 608(CX), Y9, Y7
	VPMULUDQ 672(CX), Y9, Y9
	VPADDQ   Y0, Y1, Y0
	VPADDQ   Y2, Y3, Y2
	VPADDQ   Y4, Y5, Y4
	VPADDQ   Y6, Y7, Y6
	VPADDQ   Y8, Y9, Y8

	// z0 += m(x1_2,y9_19)
	// z2 += m(x1_2,y1)
	// z4 += m(x1_2,y3)
	// z6 += m(x1_2,y5)
	// z8 += m(x1_2,y7)
	VMOVDQA  192(CX), Y9
	VPADDD   Y9, Y9, Y9
	VPMULUDQ Y14, Y9, Y1
	VPMULUDQ 512(CX), Y9, Y3
	VPMULUDQ 576(CX), Y9, Y5
	VPMULUDQ 640(CX), Y9, Y7
	VPMULUDQ 704(CX), Y9, Y9
	VPADDQ   Y0, Y1, Y0
	VPADDQ   Y2, Y3, Y2
	VPADDQ   Y4, Y5, Y4
	VPADDQ   Y6, Y7, Y6
	VPADDQ   Y8, Y9, Y8

	// z0 += m(x0,y0)
	// z2 += m(x0,y2)
	// z4 += m(x0,y4)
	// z6 += m(x0,y6)
	// z8 += m(x0,y8)
	VMOVDQA  160(CX), Y9
	VPMULUDQ 480(CX), Y9, Y1
	VPMULUDQ 544(CX), Y9, Y3
	VPMULUDQ 608(CX), Y9, Y5
	VPMULUDQ 672(CX), Y9, Y7
	VPMULUDQ 736(CX), Y9, Y9
	VPADDQ   Y0, Y1, Y0
	VPADDQ   Y2, Y3, Y2
	VPADDQ   Y4, Y5, Y4
	VPADDQ   Y6, Y7, Y6
	VPADDQ   Y8, Y9, Y8

	// Spill the completed z0, z2, z4, z6, z8 onto the stack
	VMOVDQA Y0, (CX)
	VMOVDQA Y2, 32(CX)
	VMOVDQA Y4, 64(CX)
	VMOVDQA Y6, 96(CX)
	VMOVDQA Y8, 128(CX)

	// Handle odd z vectors

	// z1 = m(x9,y2_19)
	// z3 = m(x9,y4_19)
	// z5 = m(x9,y6_19)
	// z7 = m(x9,y8_19)
	// z9 = m(x9,y0)
	VMOVDQA  448(CX), Y0
	VPMULUDQ 800(CX), Y0, Y1
	VPMULUDQ 864(CX), Y0, Y3
	VPMULUDQ Y11, Y0, Y5
	VPMULUDQ Y13, Y0, Y7
	VPMULUDQ 480(CX), Y0, Y9

	// z1 += m(x8,y3_19)
	// z3 += m(x8,y5_19)
	// z5 += m(x8,y7_19)
	// z7 += m(x8,y9_19)
	// z9 += m(x8,y1)
	VMOVDQA  416(CX), Y8
	VPMULUDQ 832(CX), Y8, Y0
	VPMULUDQ Y10, Y8, Y2
	VPMULUDQ Y12, Y8, Y4
	VPMULUDQ Y14, Y8, Y6
	VPMULUDQ 512(CX), Y8, Y8
	VPADDQ   Y1, Y0, Y1
	VPADDQ   Y3, Y2, Y3
	VPADDQ   Y5, Y4, Y5
	VPADDQ   Y7, Y6, Y7
	VPADDQ   Y9, Y8, Y9

	// z1 += m(x7,y4_19)
	// z3 += m(x7,y6_19)
	// z5 += m(x7,y8_19)
	// z7 += m(x7,y0)
	// z9 += m(x7,y2)
	VMOVDQA  384(CX), Y8
	VPMULUDQ 864(CX), Y8, Y0
	VPMULUDQ Y11, Y8, Y2
	VPMULUDQ Y13, Y8, Y4
	VPMULUDQ 480(CX), Y8, Y6
	VPMULUDQ 544(CX), Y8, Y8
	VPADDQ   Y1, Y0, Y1
	VPADDQ   Y3, Y2, Y3
	VPADDQ   Y5, Y4, Y5
	VPADDQ   Y7, Y6, Y7
	VPADDQ   Y9, Y8, Y9

	// z1 += m(x6,y5_19)
	// z3 += m(x6,y7_19)
	// z5 += m(x6,y9_19)
	// z7 += m(x6,y1)
	// z9 += m(x6,y3)
	VMOVDQA  352(CX), Y8
	VPMULUDQ Y10, Y8, Y0
	VPMULUDQ Y12, Y8, Y2
	VPMULUDQ Y14, Y8, Y4
	VPMULUDQ 512(CX), Y8, Y6
	VPMULUDQ 576(CX), Y8, Y8
	VPADDQ   Y1, Y0, Y1
	VPADDQ   Y3, Y2, Y3
	VPADDQ   Y5, Y4, Y5
	VPADDQ   Y7, Y6, Y7
	VPADDQ   Y9, Y8, Y9

	// z1 += m(x5,y6_19)
	// z3 += m(x5,y8_19)
	// z5 += m(x5,y0)
	// z7 += m(x5,y2)
	// z9 += m(x5,y4)
	VMOVDQA  320(CX), Y8
	VPMULUDQ Y11, Y8, Y0
	VPMULUDQ Y13, Y8, Y2
	VPMULUDQ 480(CX), Y8, Y4
	VPMULUDQ 544(CX), Y8, Y6
	VPMULUDQ 608(CX), Y8, Y8
	VPADDQ   Y1, Y0, Y1
	VPADDQ   Y3, Y2, Y3
	VPADDQ   Y5, Y4, Y5
	VPADDQ   Y7, Y6, Y7
	VPADDQ   Y9, Y8, Y9

	// z1 += m(x4,y7_19)
	// z3 += m(x4,y9_19)
	// z5 += m(x4,y1)
	// z7 += m(x4,y3)
	// z9 += m(x4,y5)
	VMOVDQA  288(CX), Y8
	VPMULUDQ Y12, Y8, Y0
	VPMULUDQ Y14, Y8, Y2
	VPMULUDQ 512(CX), Y8, Y4
	VPMULUDQ 576(CX), Y8, Y6
	VPMULUDQ 640(CX), Y8, Y8
	VPADDQ   Y1, Y0, Y1
	VPADDQ   Y3, Y2, Y3
	VPADDQ   Y5, Y4, Y5
	VPADDQ   Y7, Y6, Y7
	VPADDQ   Y9, Y8, Y9

	// z1 += m(x3,y8_19)
	// z3 += m(x3,y0)
	// z5 += m(x3,y2)
	// z7 += m(x3,y4)
	// z9 += m(x3,y6)
	VMOVDQA  256(CX), Y8
	VPMULUDQ Y13, Y8, Y0
	VPMULUDQ 480(CX), Y8, Y2
	VPMULUDQ 544(CX), Y8, Y4
	VPMULUDQ 608(CX), Y8, Y6
	VPMULUDQ 672(CX), Y8, Y8
	VPADDQ   Y1, Y0, Y1
	VPADDQ   Y3, Y2, Y3
	VPADDQ   Y5, Y4, Y5
	VPADDQ   Y7, Y6, Y7
	VPADDQ   Y9, Y8, Y9

	// z1 += m(x2,y9_19)
	// z3 += m(x2,y1)
	// z5 += m(x2,y3)
	// z7 += m(x2,y5)
	// z9 += m(x2,y7)
	VMOVDQA  224(CX), Y8
	VPMULUDQ Y14, Y8, Y0
	VPMULUDQ 512(CX), Y8, Y2
	VPMULUDQ 576(CX), Y8, Y4
	VPMULUDQ 640(CX), Y8, Y6
	VPMULUDQ 704(CX), Y8, Y8
	VPADDQ   Y1, Y0, Y1
	VPADDQ   Y3, Y2, Y3
	VPADDQ   Y5, Y4, Y5
	VPADDQ   Y7, Y6, Y7
	VPADDQ   Y9, Y8, Y9

	// z1 += m(x1,y0)
	// z3 += m(x1,y2)
	// z5 += m(x1,y4)
	// z7 += m(x1,y6)
	// z9 += m(x1,y8)
	VMOVDQA  192(CX), Y8
	VPMULUDQ 480(CX), Y8, Y0
	VPMULUDQ 544(CX), Y8, Y2
	VPMULUDQ 608(CX), Y8, Y4
	VPMULUDQ 672(CX), Y8, Y6
	VPMULUDQ 736(CX), Y8, Y8
	VPADDQ   Y1, Y0, Y1
	VPADDQ   Y3, Y2, Y3
	VPADDQ   Y5, Y4, Y5
	VPADDQ   Y7, Y6, Y7
	VPADDQ   Y9, Y8, Y9

	// z1 += m(x0,y1)
	// z3 += m(x0,y3)
	// z5 += m(x0,y5)
	// z7 += m(x0,y7)
	// z9 += m(x0,y9)
	VMOVDQA  160(CX), Y8
	VPMULUDQ 512(CX), Y8, Y0
	VPMULUDQ 576(CX), Y8, Y2
	VPMULUDQ 640(CX), Y8, Y4
	VPMULUDQ 704(CX), Y8, Y6
	VPMULUDQ 768(CX), Y8, Y8
	VPADDQ   Y1, Y0, Y1
	VPADDQ   Y3, Y2, Y3
	VPADDQ   Y5, Y4, Y5
	VPADDQ   Y7, Y6, Y7
	VPADDQ   Y9, Y8, Y9

	// Restore the completed z0, z2, z4, z6, z8 from the stack
	VMOVDQA (CX), Y0
	VMOVDQA 32(CX), Y2
	VMOVDQA 64(CX), Y4
	VMOVDQA 96(CX), Y6
	VMOVDQA 128(CX), Y8

	// Reduce
	VMOVDQA low_25_bit_mask<>+0(SB), Y10
	VMOVDQA low_26_bit_mask<>+0(SB), Y11
	VMOVDQA v19<>+0(SB), Y12

	// Perform two halves of the carry chain in parallel

	// Carry z[0]/z[4]
	VPSRLQ $0x1a, Y0, Y13
	VPSRLQ $0x1a, Y4, Y14
	VPADDQ Y1, Y13, Y1
	VPADDQ Y5, Y14, Y5
	VPAND  Y11, Y0, Y0
	VPAND  Y11, Y4, Y4

	// Carry z[1]/z[5]
	VPSRLQ $0x19, Y1, Y13
	VPSRLQ $0x19, Y5, Y14
	VPADDQ Y2, Y13, Y2
	VPADDQ Y6, Y14, Y6
	VPAND  Y10, Y1, Y1
	VPAND  Y10, Y5, Y5

	// Carry z[2]/z[6]
	VPSRLQ $0x1a, Y2, Y13
	VPSRLQ $0x1a, Y6, Y14
	VPADDQ Y3, Y13, Y3
	VPADDQ Y7, Y14, Y7
	VPAND  Y11, Y2, Y2
	VPAND  Y11, Y6, Y6

	// Carry z[3]/z[7]
	VPSRLQ $0x19, Y3, Y13
	VPSRLQ $0x19, Y7, Y14
	VPADDQ Y4, Y13, Y4
	VPADDQ Y8, Y14, Y8
	VPAND  Y10, Y3, Y3
	VPAND  Y10, Y7, Y7

	// Carry z[4]/z[8]
	VPSRLQ $0x1a, Y4, Y13
	VPSRLQ $0x1a, Y8, Y14
	VPADDQ Y5, Y13, Y5
	VPADDQ Y9, Y14, Y9
	VPAND  Y11, Y4, Y4
	VPAND  Y11, Y8, Y8

	// Do the final carry
	VPSRLQ   $0x19, Y9, Y13
	VPAND    Y10, Y9, Y9
	VPAND    Y11, Y13, Y10
	VPSRLQ   $0x1a, Y13, Y13
	VPMULUDQ Y12, Y10, Y10
	VPMULUDQ Y12, Y13, Y13
	VPADDQ   Y0, Y10, Y0
	VPADDQ   Y1, Y13, Y1
	VPSRLQ   $0x1a, Y0, Y10
	VPADDQ   Y1, Y10, Y1
	VPAND    Y11, Y0, Y0

	// Repack 64-bit lanes into 32-bit lanes
	VPSHUFD  $0xd8, Y0, Y0
	VPSHUFD  $0x8d, Y1, Y1
	VPBLENDD $0xcc, Y1, Y0, Y0
	VPSHUFD  $0xd8, Y2, Y2
	VPSHUFD  $0x8d, Y3, Y3
	VPBLENDD $0xcc, Y3, Y2, Y2
	VPSHUFD  $0xd8, Y4, Y4
	VPSHUFD  $0x8d, Y5, Y5
	VPBLENDD $0xcc, Y5, Y4, Y4
	VPSHUFD  $0xd8, Y6, Y6
	VPSHUFD  $0x8d, Y7, Y7
	VPBLENDD $0xcc, Y7, Y6, Y6
	VPSHUFD  $0xd8, Y8, Y8
	VPSHUFD  $0x8d, Y9, Y9
	VPBLENDD $0xcc, Y9, Y8, Y8

	// Write out the result
	VMOVDQU Y0, (AX)
	VMOVDQU Y2, 32(AX)
	VMOVDQU Y4, 64(AX)
	VMOVDQU Y6, 96(AX)
	VMOVDQU Y8, 128(AX)
	VZEROUPPER
	RET

// func vecSquareAndNegateD_AVX2(out *fieldElement2625x4)
// Requires: AVX, AVX2
TEXT ·vecSquareAndNegateD_AVX2(SB), $544-8
	MOVQ out+0(FP), AX

	// Align the stack on a 64 byte boundary (cache line aligned)
	MOVQ SP, CX
	ADDQ $0x40, CX
	ANDQ $0xffffffc0, CX

	// Load, unpack, and spill out (x)
	VPXOR      Y10, Y10, Y10
	VMOVDQU    (AX), Y0
	VPUNPCKHDQ Y10, Y0, Y1
	VPUNPCKLDQ Y10, Y0, Y0
	VMOVDQU    32(AX), Y2
	VPUNPCKHDQ Y10, Y2, Y3
	VPUNPCKLDQ Y10, Y2, Y2
	VMOVDQU    64(AX), Y4
	VPUNPCKHDQ Y10, Y4, Y5
	VPUNPCKLDQ Y10, Y4, Y4
	VMOVDQU    96(AX), Y6
	VPUNPCKHDQ Y10, Y6, Y7
	VPUNPCKLDQ Y10, Y6, Y6
	VMOVDQU    128(AX), Y8
	VPUNPCKHDQ Y10, Y8, Y9
	VPUNPCKLDQ Y10, Y8, Y8
	VMOVDQU    Y0, 160(CX)
	VMOVDQU    Y1, 192(CX)
	VMOVDQU    Y2, 224(CX)
	VMOVDQU    Y3, 256(CX)
	VMOVDQU    Y4, 288(CX)
	VMOVDQU    Y5, 320(CX)
	VMOVDQU    Y6, 352(CX)
	VMOVDQU    Y7, 384(CX)
	VMOVDQU    Y8, 416(CX)
	VMOVDQU    Y9, 448(CX)

	// Precompute (x1, x3, x5, x7) * 2
	VMOVDQA v19<>+0(SB), Y10
	VPADDD  Y1, Y1, Y11
	VPADDD  Y3, Y3, Y12
	VPADDD  Y5, Y5, Y13
	VPADDD  Y7, Y7, Y14

	// z0 = m(x1_2,x9_19)
	// z1 = m(x2,x9_19)
	// z2 = m(x3_2,x9_19)
	// z3 = m(x4,x9_19)
	// z4 = m(x5_2,x9_19)
	// z5 = m(x6,x9_19)
	// z6 = m(x7_2,x9_19)
	// z7 = m(x8,x9_19)
	// z8 = m(x9,x9_19)
	VPMULUDQ Y10, Y9, Y15
	VPMULUDQ Y11, Y15, Y0
	VPMULUDQ Y2, Y15, Y1
	VPMULUDQ Y12, Y15, Y2
	VPMULUDQ Y4, Y15, Y3
	VPMULUDQ Y13, Y15, Y4
	VPMULUDQ Y6, Y15, Y5
	VPMULUDQ Y14, Y15, Y6
	VPMULUDQ Y8, Y15, Y7
	VPMULUDQ Y9, Y15, Y8

	// (z5, z6, z7, z8) <<= 1 (results spilled)
	VPADDQ  Y5, Y5, Y5
	VPADDQ  Y6, Y6, Y6
	VPADDQ  Y7, Y7, Y7
	VPADDQ  Y8, Y8, Y8
	VMOVDQA Y5, (CX)
	VMOVDQA Y6, 32(CX)
	VMOVDQA Y7, 64(CX)
	VMOVDQA Y8, 96(CX)

	// z0 += m(x3_2,x7_19)
	// z1 += m(x4,x7_19)
	// z2 += m(x5_2,x7_19)
	// z3 += m(x6,x7_19)
	// z4 += m(x7,x7_19)
	VMOVDQA  384(CX), Y9
	VPMULUDQ Y10, Y9, Y15
	VPMULUDQ Y12, Y15, Y5
	VPMULUDQ 288(CX), Y15, Y6
	VPMULUDQ Y13, Y15, Y7
	VPMULUDQ 352(CX), Y15, Y8
	VPMULUDQ Y9, Y15, Y9
	VPADDQ   Y0, Y5, Y0
	VPADDQ   Y1, Y6, Y1
	VPADDQ   Y2, Y7, Y2
	VPADDQ   Y3, Y8, Y3
	VPADDQ   Y4, Y9, Y4

	// z0 += m(x5,x5_19)
	VMOVDQA  320(CX), Y5
	VPMULUDQ Y10, Y5, Y6
	VPMULUDQ Y5, Y6, Y5
	VPADDQ   Y0, Y5, Y0

	// (z0 .. z4) <<= 1
	VPADDQ Y0, Y0, Y0
	VPADDQ Y1, Y1, Y1
	VPADDQ Y2, Y2, Y2
	VPADDQ Y3, Y3, Y3
	VPADDQ Y4, Y4, Y4

	// At this point:
	// z0 = ((m(x1_2,x9_19) + m(x3_2,x7_19) + m(x5,x5_19)) << 1)
	// z1 = ((m(x2,x9_19)   + m(x4,x7_19))                 << 1)
	// z2 = ((m(x3_2,x9_19) + m(x5_2,x7_19))               << 1)
	// z3 = ((m(x4,x9_19)   + m(x6,x7_19))                 << 1)
	// z4 = ((m(x5_2,x9_19) + m(x7,x7_19))                 << 1)
	// z5 = ((m(x6,x9_19))                                 << 1) (spilled)
	// z6 = ((m(x7_2,x9_19))                               << 1) (spilled)
	// z7 = ((m(x8,x9_19))                                 << 1) (spilled)
	// z8 = ((m(x9,x9_19))                                 << 1) (spilled)
	// z9 = undefined

	// z2 += m(x6,x6_19)
	// z4 += m(x6_2,x8_19)
	VMOVDQA  352(CX), Y6
	VPMULUDQ 416(CX), Y10, Y5
	VPMULUDQ Y6, Y10, Y7
	VPADDD   Y6, Y6, Y8
	VPMULUDQ Y6, Y7, Y6
	VPMULUDQ Y8, Y5, Y8
	VPADDQ   Y2, Y6, Y2
	VPADDQ   Y4, Y8, Y4

	// z1 += m(x5_2,x6_19)
	// z3 += m(x5_2,x8_19)
	// z0 += m(x4_2,x6_19)
	// z2 += m(x4_2,x8_19)
	VMOVDQA  288(CX), Y8
	VPADDQ   Y8, Y8, Y8
	VPMULUDQ Y13, Y7, Y6
	VPMULUDQ Y13, Y5, Y9
	VPMULUDQ Y8, Y7, Y7
	VPMULUDQ Y8, Y5, Y8
	VPADDQ   Y1, Y6, Y1
	VPADDQ   Y3, Y9, Y3
	VPADDQ   Y0, Y7, Y0
	VPADDQ   Y2, Y8, Y2

	// z0 += m(x2_2,x8_19)
	// z1 += m(x3_2,x8_19)
	// z4 += m(x2,x2)
	VMOVDQA  224(CX), Y6
	VPADDD   Y6, Y6, Y7
	VPMULUDQ Y7, Y5, Y7
	VPMULUDQ Y12, Y5, Y8
	VPMULUDQ Y6, Y6, Y9
	VPADDQ   Y0, Y7, Y0
	VPADDQ   Y1, Y8, Y1
	VPADDQ   Y4, Y9, Y4

	// z2 += m(x1_2,x1)
	// z3 += m(x1_2,x2)
	// z4 += m(x1_2,x3_2)
	VPMULUDQ 192(CX), Y11, Y7
	VPMULUDQ Y6, Y11, Y8
	VPMULUDQ Y12, Y11, Y9
	VPADDQ   Y2, Y7, Y2
	VPADDQ   Y3, Y8, Y3
	VPADDQ   Y4, Y9, Y4

	// z0 += m(x0,x0)
	// z1 += m(x0_2,x1)
	// z2 += m(x0_2,x2)
	// z3 += m(x0_2,x3)
	// z4 += m(x0_2,x4)
	// Note: (z0 .. z4) done at this point
	VMOVDQA  160(CX), Y7
	VPADDD   Y7, Y7, Y9
	VPMULUDQ Y7, Y7, Y7
	VPMULUDQ 192(CX), Y9, Y8
	VPMULUDQ Y6, Y9, Y6
	VPMULUDQ 256(CX), Y9, Y10
	VPMULUDQ 288(CX), Y9, Y15
	VPADDQ   Y0, Y7, Y0
	VPADDQ   Y1, Y8, Y1
	VPADDQ   Y2, Y6, Y2
	VPADDQ   Y3, Y10, Y3
	VPADDQ   Y4, Y15, Y4

	// z5 += m(x0_2,x5)
	// z6 += m(x0_2,x6)
	// z7 += m(x0_2,x7)
	// z8 += m(x0_2,x8)
	// z9 = m(x0_2,x9)
	VPMULUDQ 320(CX), Y9, Y6
	VPMULUDQ 352(CX), Y9, Y7
	VPMULUDQ 384(CX), Y9, Y8
	VPMULUDQ 416(CX), Y9, Y10
	VPMULUDQ 448(CX), Y9, Y9
	VPADDQ   (CX), Y6, Y6
	VPADDQ   32(CX), Y7, Y7
	VPADDQ   64(CX), Y8, Y8
	VPADDQ   96(CX), Y10, Y10

	// Now that (z0 .. z4) are done, and we unspilled (z5 .. z8) as
	// part of the previous group of multiply/adds, we spill (z0 .. z4)
	// to free up registers.
	VMOVDQA Y0, (CX)
	VMOVDQA Y1, 32(CX)
	VMOVDQA Y2, 64(CX)
	VMOVDQA Y3, 96(CX)
	VMOVDQA Y4, 128(CX)

	// z5 += m(x1_2,x4)
	// z6 += m(x1_2,x5_2)
	// z7 += m(x1_2,x6)
	// z8 += m(x1_2,x7_2)
	// z9 += m(x1_2,x8)
	VPMULUDQ 288(CX), Y11, Y0
	VPMULUDQ Y13, Y11, Y1
	VPMULUDQ 352(CX), Y11, Y2
	VPMULUDQ Y14, Y11, Y3
	VPMULUDQ 416(CX), Y11, Y4
	VPADDQ   Y6, Y0, Y6
	VPADDQ   Y7, Y1, Y7
	VPADDQ   Y8, Y2, Y8
	VPADDQ   Y10, Y3, Y10
	VPADDQ   Y9, Y4, Y9

	// z5 += m(x2_2,x3)
	// z6 += m(x2_2,x4)
	// z7 += m(x2_2,x5)
	// z8 += m(x2_2,x6)
	// z9 += m(x2_2,x7)
	VMOVDQA  288(CX), Y11
	VMOVDQA  224(CX), Y4
	VPADDD   Y4, Y4, Y4
	VPMULUDQ 256(CX), Y4, Y0
	VPMULUDQ Y11, Y4, Y1
	VPMULUDQ 320(CX), Y4, Y2
	VPMULUDQ 352(CX), Y4, Y3
	VPMULUDQ 384(CX), Y4, Y4
	VPADDQ   Y6, Y0, Y6
	VPADDQ   Y7, Y1, Y7
	VPADDQ   Y8, Y2, Y8
	VPADDQ   Y10, Y3, Y10
	VPADDQ   Y9, Y4, Y9

	// z6 += m(x3_2,x3)
	// z7 += m(x3_2,x4)
	// z8 += m(x3_2,x5_2)
	// z9 += m(x3_2,x6)
	VPMULUDQ 256(CX), Y12, Y1
	VPMULUDQ Y11, Y12, Y2
	VPMULUDQ Y13, Y12, Y3
	VPMULUDQ 352(CX), Y12, Y4
	VPADDQ   Y7, Y1, Y7
	VPADDQ   Y8, Y2, Y8
	VPADDQ   Y10, Y3, Y10
	VPADDQ   Y9, Y4, Y9

	// z5 += m(x7_2,x8_19)
	// z6 += m(x8,x8_19)
	// z8 += m(x4,x4)
	// z9 += m(x4_2,x5)
	VPADDD   Y11, Y11, Y2
	VPMULUDQ Y14, Y5, Y0
	VPMULUDQ 416(CX), Y5, Y1
	VPMULUDQ Y11, Y11, Y3
	VPMULUDQ 320(CX), Y2, Y4
	VPADDQ   Y6, Y0, Y6
	VPADDQ   Y7, Y1, Y7
	VPADDQ   Y10, Y3, Y10
	VPADDQ   Y9, Y4, Y9

	// Restore the completed (z0, .., z4) from the stack
	VMOVDQA (CX), Y0
	VMOVDQA 32(CX), Y1
	VMOVDQA 64(CX), Y2
	VMOVDQA 96(CX), Y3
	VMOVDQA 128(CX), Y4

	// Negate D

	// Negate even D values
	VMOVDQA  low_p_37<>+0(SB), Y5
	VMOVDQA  even_p_37<>+0(SB), Y11
	VPSUBQ   Y0, Y5, Y5
	VPSUBQ   Y2, Y11, Y12
	VPSUBQ   Y4, Y11, Y13
	VPSUBQ   Y7, Y11, Y14
	VPSUBQ   Y10, Y11, Y11
	VPBLENDD $0xc0, Y5, Y0, Y0
	VPBLENDD $0xc0, Y12, Y2, Y2
	VPBLENDD $0xc0, Y13, Y4, Y4
	VPBLENDD $0xc0, Y14, Y7, Y7
	VPBLENDD $0xc0, Y11, Y10, Y10

	// Negate odd D values
	VMOVDQA  odd_p_37<>+0(SB), Y5
	VPSUBQ   Y1, Y5, Y11
	VPSUBQ   Y3, Y5, Y12
	VPSUBQ   Y6, Y5, Y13
	VPSUBQ   Y8, Y5, Y14
	VPSUBQ   Y9, Y5, Y5
	VPBLENDD $0xc0, Y11, Y1, Y1
	VPBLENDD $0xc0, Y12, Y3, Y3
	VPBLENDD $0xc0, Y13, Y6, Y6
	VPBLENDD $0xc0, Y14, Y8, Y8
	VPBLENDD $0xc0, Y5, Y9, Y9

	// Reduce
	VMOVDQA low_25_bit_mask<>+0(SB), Y5
	VMOVDQA low_26_bit_mask<>+0(SB), Y11
	VMOVDQA v19<>+0(SB), Y12

	// Perform two halves of the carry chain in parallel

	// Carry z[0]/z[4]
	VPSRLQ $0x1a, Y0, Y13
	VPSRLQ $0x1a, Y4, Y14
	VPADDQ Y1, Y13, Y1
	VPADDQ Y6, Y14, Y6
	VPAND  Y11, Y0, Y0
	VPAND  Y11, Y4, Y4

	// Carry z[1]/z[5]
	VPSRLQ $0x19, Y1, Y13
	VPSRLQ $0x19, Y6, Y14
	VPADDQ Y2, Y13, Y2
	VPADDQ Y7, Y14, Y7
	VPAND  Y5, Y1, Y1
	VPAND  Y5, Y6, Y6

	// Carry z[2]/z[6]
	VPSRLQ $0x1a, Y2, Y13
	VPSRLQ $0x1a, Y7, Y14
	VPADDQ Y3, Y13, Y3
	VPADDQ Y8, Y14, Y8
	VPAND  Y11, Y2, Y2
	VPAND  Y11, Y7, Y7

	// Carry z[3]/z[7]
	VPSRLQ $0x19, Y3, Y13
	VPSRLQ $0x19, Y8, Y14
	VPADDQ Y4, Y13, Y4
	VPADDQ Y10, Y14, Y10
	VPAND  Y5, Y3, Y3
	VPAND  Y5, Y8, Y8

	// Carry z[4]/z[8]
	VPSRLQ $0x1a, Y4, Y13
	VPSRLQ $0x1a, Y10, Y14
	VPADDQ Y6, Y13, Y6
	VPADDQ Y9, Y14, Y9
	VPAND  Y11, Y4, Y4
	VPAND  Y11, Y10, Y10

	// Do the final carry
	VPSRLQ   $0x19, Y9, Y13
	VPAND    Y5, Y9, Y9
	VPAND    Y11, Y13, Y5
	VPSRLQ   $0x1a, Y13, Y13
	VPMULUDQ Y12, Y5, Y5
	VPMULUDQ Y12, Y13, Y13
	VPADDQ   Y0, Y5, Y0
	VPADDQ   Y1, Y13, Y1
	VPSRLQ   $0x1a, Y0, Y5
	VPADDQ   Y1, Y5, Y1
	VPAND    Y11, Y0, Y0

	// Repack 64-bit lanes into 32-bit lanes
	VPSHUFD  $0xd8, Y0, Y0
	VPSHUFD  $0x8d, Y1, Y1
	VPBLENDD $0xcc, Y1, Y0, Y0
	VPSHUFD  $0xd8, Y2, Y2
	VPSHUFD  $0x8d, Y3, Y3
	VPBLENDD $0xcc, Y3, Y2, Y2
	VPSHUFD  $0xd8, Y4, Y4
	VPSHUFD  $0x8d, Y6, Y6
	VPBLENDD $0xcc, Y6, Y4, Y4
	VPSHUFD  $0xd8, Y7, Y7
	VPSHUFD  $0x8d, Y8, Y8
	VPBLENDD $0xcc, Y8, Y7, Y7
	VPSHUFD  $0xd8, Y10, Y10
	VPSHUFD  $0x8d, Y9, Y9
	VPBLENDD $0xcc, Y9, Y10, Y10

	// Write out the result
	VMOVDQU Y0, (AX)
	VMOVDQU Y2, 32(AX)
	VMOVDQU Y4, 64(AX)
	VMOVDQU Y7, 96(AX)
	VMOVDQU Y10, 128(AX)
	VZEROUPPER
	RET
